# Project_1_Twitter_Scraper
Scrapping Data from Twitter based on user input.

# Requirements
I used snscrape, pymongo, pandas, streamlit, datetime modules for this project.
snscrape: snscrape is used to scrape the data from the social media. From Twitter it scrapes things like user, id, content, likescount, retweetCount, hashtags,etc.
  pip3 install snscrape --> used to install snscrape package in python.
pymongo is used to connect python with mongodb database.
  pip3 install pymongo  --> used to install pymongo package in python.
Pandas is used to create a dataframe. 
  pip3 install pandas  --> used to install pandas package in python.
Streamlit is used to create a web application.
  pip3 install streamlit  --> used to install streamli package in python.

# To get user inputs
sntwitter.TwitterSearchScraper(Text) used to scrape the data from twitter.
To get User input

      text=st.text_input('Enter the Search_keyword') 
      until_date=st.text_input('Until: YYYY-MM-DD') 
      since_date=st.text_input('Since: YYYY-MM-DD')
      limit_range=st.text_input('Limit_range: number')
      scrapped_data = sntwitter.TwitterSearchScraper(text+' '+'until:'+until_date+' '+'since:'+since_date).get_items()
     
     for i in scrapped_data:
    # appending 'date','id','url','content','user','replyCount','retweetCount','lang','source','likeCount' attributes scrapped from twitter
          a.append(i.date)
          b.append(i.id)
          c.append(i.url)
          d.append(i.content)
          e.append(i.user)
          f.append(i.replyCount)
          g.append(i.retweetCount)
          h.append(i.lang)
          x.append(i.source)
          y.append(i.likeCount)
          count=count+1
          if count>=int(limit_range):
              break


# Creating Dataframe
      data = {'date': a, 'id': b, 'url': c, 'content': d, 'user': e, 'replyCount': f, 'retweetCount': g, 'lang': h,'source': x, 'likeCount': y}
      df = pd.DataFrame(data)
      # Create a download button in the name 'Download CSV' to download the data in CSV format
      st.download_button("Download CSV",df.to_csv(),file_name='Twitter_data.csv',mime='text/csv')
      
#Uploading the scrapped data into database
        To upload a document into a database we need key and values.
        Key is Generated by concatenating search keyword+current timestamp.
        
        now = datetime.now()                  # current time
        timestamp1 = datetime.timestamp(now)  # converting the datetime object into a timestamp
        myclient = pymongo.MongoClient("mongodb://localhost:27017/")
        mydb = myclient["Twitter_database"]  # Creating a database named 'Twitter_database'
        mycol = mydb["Twitter_datas"]  # Creating a collection named 'Twitter_datas'
        key = text + str(timestamp1)  # concatenating search_keyword with timestamp to make it as a keyword for the database document
        # eg:({“text+current Timestamp”: [{1000 Scraped data from past 100 days }]})
        x = mycol.insert_one({key: [cd]})  # Inserting the datas which is stored in a variable 'c' with help of created key
        st.write('Data uploaded successfully')
  
  



